import os
import logging
from typing import List, Optional
from pathlib import Path

# LlamaIndex imports
from llama_index.core import VectorStoreIndex, Document, Settings, StorageContext
from llama_index.core.node_parser import SentenceSplitter
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.groq import Groq
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.postprocessor import SimilarityPostprocessor

# Chroma imports
import chromadb
from chromadb.config import Settings as ChromaSettings

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RAGChatbot:
    """
    Retrieval Augmented Generation Chatbot for Mental Models in Marathi
    Uses LlamaIndex with ChromaDB for vector storage and Groq API for Gemma model
    """
    
    def __init__(self, data_directory: str, groq_api_key: str, 
                 model_name: str = "gemma-7b-it", 
                 embedding_model: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        """
        Initialize the RAG chatbot
        
        Args:
            data_directory: Path to directory containing mental models data files
            groq_api_key: Groq API key for accessing Gemma models
            model_name: Groq model name to use
            embedding_model: HuggingFace embedding model for multilingual support
        """
        self.data_directory = data_directory
        self.groq_api_key = groq_api_key
        self.model_name = model_name
        self.embedding_model = embedding_model
        
        # Initialize components
        self._setup_llm()
        self._setup_embeddings()
        self._setup_vector_store()
        self._load_documents()
        self._create_index()
        self._setup_query_engine()
        
        logger.info("RAG Chatbot initialized successfully")
    
    def _setup_llm(self):
        """Setup Groq LLM"""
        try:
            self.llm = Groq(
                model=self.model_name,
                api_key=self.groq_api_key,
                temperature=0.1
            )
            Settings.llm = self.llm
            logger.info(f"LLM setup completed with model: {self.model_name}")
        except Exception as e:
            logger.error(f"Error setting up LLM: {e}")
            raise
    
    def _setup_embeddings(self):
        """Setup multilingual embeddings"""
        try:
            self.embed_model = HuggingFaceEmbedding(
                model_name=self.embedding_model,
                cache_folder="./embeddings_cache"
            )
            Settings.embed_model = self.embed_model
            logger.info(f"Embeddings setup completed with model: {self.embedding_model}")
        except Exception as e:
            logger.error(f"Error setting up embeddings: {e}")
            raise
    
    def _setup_vector_store(self):
        """Setup ChromaDB vector store"""
        try:
            # Initialize Chroma client
            chroma_client = chromadb.PersistentClient(
                path="./chroma_db",
                settings=ChromaSettings(anonymized_telemetry=False)
            )
            
            # Get or create collection
            collection_name = "mental_models_marathi"
            try:
                chroma_collection = chroma_client.get_collection(collection_name)
                logger.info(f"Loaded existing collection: {collection_name}")
            except:
                chroma_collection = chroma_client.create_collection(collection_name)
                logger.info(f"Created new collection: {collection_name}")
            
            # Create vector store
            self.vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
            self.storage_context = StorageContext.from_defaults(vector_store=self.vector_store)
            
        except Exception as e:
            logger.error(f"Error setting up vector store: {e}")
            raise
    
    def _load_documents(self):
        """Load documents from data directory"""
        try:
            self.documents = []
            data_path = Path(self.data_directory)
            
            # Supported file extensions
            supported_extensions = ['.txt', '.md', '.json']
            
            for file_path in data_path.rglob('*'):
                if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            if content.strip():  # Only add non-empty files
                                doc = Document(
                                    text=content,
                                    metadata={
                                        "filename": file_path.name,
                                        "filepath": str(file_path),
                                        "file_type": file_path.suffix
                                    }
                                )
                                self.documents.append(doc)
                    except Exception as e:
                        logger.warning(f"Error reading file {file_path}: {e}")
            
            logger.info(f"Loaded {len(self.documents)} documents")
            
            if not self.documents:
                raise ValueError("No valid documents found in the data directory")
                
        except Exception as e:
            logger.error(f"Error loading documents: {e}")
            raise
    
    def _create_index(self):
        """Create vector index"""
        try:
            # Setup text splitter for better chunking
            text_splitter = SentenceSplitter(
                chunk_size=512,
                chunk_overlap=50
            )
            Settings.text_splitter = text_splitter
            
            # Create index
            self.index = VectorStoreIndex.from_documents(
                self.documents,
                storage_context=self.storage_context,
                show_progress=True
            )
            logger.info("Vector index created successfully")
            
        except Exception as e:
            logger.error(f"Error creating index: {e}")
            raise
    
    def _setup_query_engine(self):
        """Setup query engine with custom prompt"""
        try:
            # Create retriever
            retriever = VectorIndexRetriever(
                index=self.index,
                similarity_top_k=5
            )
            
            # Create query engine with postprocessor
            self.query_engine = RetrieverQueryEngine(
                retriever=retriever,
                node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)]
            )
            
            logger.info("Query engine setup completed")
            
        except Exception as e:
            logger.error(f"Error setting up query engine: {e}")
            raise
    
    def get_response(self, question: str) -> str:
        """
        Get response to user question using RAG
        
        Args:
            question: User question in Marathi or English
            
        Returns:
            Generated response
        """
        try:
            # Custom prompt for mental models validation
            custom_prompt = f"""
            ‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§è‡§ï ‡§Æ‡§æ‡§®‡§∏‡§ø‡§ï ‡§Æ‡•â‡§°‡•á‡§≤‡•ç‡§∏ (Mental Models) ‡§§‡§ú‡•ç‡§û ‡§Ü‡§π‡§æ‡§§. ‡§¶‡§ø‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠‡§æ‡§µ‡§∞ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§æ‡§ö‡•á ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•ç‡§Ø‡§æ.

            ‡§®‡§ø‡§Ø‡§Æ:
            1. ‡§´‡§ï‡•ç‡§§ ‡§¶‡§ø‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠‡§æ‡§§‡•Ä‡§≤ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä‡§µ‡§∞ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•ç‡§Ø‡§æ
            2. ‡§â‡§§‡•ç‡§§‡§∞ ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§¶‡•ç‡§Ø‡§æ ‡§ú‡•á‡§µ‡•ç‡§π‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§Ü‡§π‡•á
            3. Mental model ‡§ö‡•á ‡§®‡§æ‡§µ, ‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ ‡§Ü‡§£‡§ø ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§π‡§æ‡§∞‡§ø‡§ï ‡§â‡§¶‡§æ‡§π‡§∞‡§£ ‡§¶‡•ç‡§Ø‡§æ
            4. ‡§ú‡§∞ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠‡§æ‡§§ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§®‡§∏‡•á‡§≤ ‡§§‡§∞ "‡§Æ‡§≤‡§æ ‡§Ø‡§æ ‡§µ‡§ø‡§∑‡§Ø‡§æ‡§µ‡§∞ ‡§™‡•Å‡§∞‡•á‡§∂‡•Ä ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§®‡§æ‡§π‡•Ä" ‡§Ö‡§∏‡•á ‡§∏‡§æ‡§Ç‡§ó‡§æ
            5. ‡§â‡§§‡•ç‡§§‡§∞ ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§Ü‡§£‡§ø ‡§∏‡§Æ‡§ú‡§£‡•ç‡§Ø‡§æ‡§∏‡§æ‡§∞‡§ñ‡•á ‡§Ö‡§∏‡§æ‡§µ‡•á

            ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: {question}

            ‡§ï‡•É‡§™‡§Ø‡§æ ‡§µ‡§∞‡•Ä‡§≤ ‡§®‡§ø‡§Ø‡§Æ‡§æ‡§Ç‡§®‡•Å‡§∏‡§æ‡§∞ ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•ç‡§Ø‡§æ.
            """
            
            # Get response from query engine
            response = self.query_engine.query(custom_prompt)
            
            # Validate response
            validated_response = self._validate_response(str(response), question)
            
            return validated_response
            
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            return f"‡§Æ‡§æ‡§´ ‡§ï‡§∞‡§æ, ‡§â‡§§‡•ç‡§§‡§∞ ‡§§‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§§‡§æ‡§®‡§æ ‡§§‡•ç‡§∞‡•Å‡§ü‡•Ä ‡§Ü‡§≤‡•Ä: {str(e)}"
    
    def _validate_response(self, response: str, question: str) -> str:
        """
        Validate and enhance the response
        
        Args:
            response: Generated response
            question: Original question
            
        Returns:
            Validated response
        """
        try:
            # Basic validation prompt
            validation_prompt = f"""
            ‡§π‡•á ‡§â‡§§‡•ç‡§§‡§∞ ‡§§‡§™‡§æ‡§∏‡§æ ‡§Ü‡§£‡§ø ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§Ö‡§∏‡§≤‡•ç‡§Ø‡§æ‡§∏ ‡§∏‡•Å‡§ß‡§æ‡§∞‡§æ:

            ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: {question}
            ‡§â‡§§‡•ç‡§§‡§∞: {response}

            ‡§§‡§™‡§æ‡§∏‡§£‡•Ä:
            1. ‡§â‡§§‡•ç‡§§‡§∞ ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§æ‡§∂‡•Ä ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§Ü‡§π‡•á ‡§ï‡§æ?
            2. ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§¨‡§∞‡•ã‡§¨‡§∞ ‡§Ü‡§π‡•á ‡§ï‡§æ?
            3. ‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§≠‡§æ‡§∑‡§æ ‡§Ø‡•ã‡§ó‡•ç‡§Ø ‡§Ü‡§π‡•á ‡§ï‡§æ?
            4. ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü‡•Ä‡§ï‡§∞‡§£ ‡§™‡•Å‡§∞‡•á‡§∏‡•á ‡§Ü‡§π‡•á ‡§ï‡§æ?

            ‡§∏‡•Å‡§ß‡§æ‡§∞‡§≤‡•á‡§≤‡•á ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•ç‡§Ø‡§æ:
            """
            
            # In a production environment, you might want to add another validation step
            # For now, return the original response with basic checks
            
            if len(response.strip()) < 10:
                return "‡§Æ‡§æ‡§´ ‡§ï‡§∞‡§æ, ‡§Ø‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§æ‡§µ‡§∞ ‡§Æ‡§≤‡§æ ‡§™‡•Å‡§∞‡•á‡§∂‡•Ä ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§Æ‡§ø‡§≥‡§æ‡§≤‡•Ä ‡§®‡§æ‡§π‡•Ä. ‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ö‡§ß‡§ø‡§ï ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§µ‡§ø‡§ö‡§æ‡§∞‡§æ."
            
            return response
            
        except Exception as e:
            logger.error(f"Error validating response: {e}")
            return response
    
    def get_response_with_finetuned(self, question: str, finetuned_model=None) -> str:
        """
        Get response using fine-tuned model if available
        
        Args:
            question: User question
            finetuned_model: Fine-tuned model object
            
        Returns:
            Generated response
        """
        if finetuned_model is not None:
            try:
                # Use fine-tuned model for generation
                # This would integrate with the fine-tuned model from fine_tune.py
                logger.info("Using fine-tuned model for response generation")
                # Implementation would depend on the fine-tuned model structure
                pass
            except Exception as e:
                logger.error(f"Error using fine-tuned model: {e}")
        
        # Fallback to regular RAG
        return self.get_response(question)

if __name__ == "__main__":
    """
    Test the RAG chatbot functionality
    """
    import tempfile
    import os
    
    # Test data
    test_data = """
    Sunk Cost Fallacy (‡§¨‡•Å‡§°‡§æ‡§≤‡•á‡§≤‡§æ ‡§ñ‡§∞‡•ç‡§ö ‡§ö‡•Å‡§ï‡•Ä‡§ö‡§æ ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø)
    
    ‡§π‡§æ ‡§è‡§ï ‡§Æ‡§æ‡§®‡§∏‡§ø‡§ï ‡§Æ‡•â‡§°‡•á‡§≤ ‡§Ü‡§π‡•á ‡§ú‡•ç‡§Ø‡§æ‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§Ü‡§™‡§£ ‡§Ü‡§ß‡•Ä ‡§ï‡•á‡§≤‡•á‡§≤‡§æ ‡§ñ‡§∞‡•ç‡§ö ‡§ï‡§ø‡§Ç‡§µ‡§æ ‡§ó‡•Å‡§Ç‡§§‡§µ‡§£‡•Å‡§ï‡•Ä‡§Æ‡•Å‡§≥‡•á ‡§ö‡•Å‡§ï‡•Ä‡§ö‡•á ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø ‡§ò‡•á‡§§‡•ã.
    
    ‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ: ‡§ú‡•á‡§µ‡•ç‡§π‡§æ ‡§Ü‡§™‡§£ ‡§Ü‡§ß‡•Ä ‡§ï‡•á‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§ó‡•Å‡§Ç‡§§‡§µ‡§£‡•Å‡§ï‡•Ä‡§Æ‡•Å‡§≥‡•á (‡§™‡•à‡§∏‡§æ, ‡§µ‡•á‡§≥, ‡§Æ‡•á‡§π‡§®‡§§) ‡§è‡§ñ‡§æ‡§¶‡§æ ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø ‡§ò‡•á‡§§‡•ã, ‡§ú‡§∞‡•Ä ‡§§‡•ã ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø ‡§Ü‡§§‡§æ ‡§ö‡•Å‡§ï‡•Ä‡§ö‡§æ ‡§Ö‡§∏‡§≤‡§æ ‡§§‡§∞‡•Ä.
    
    ‡§â‡§¶‡§æ‡§π‡§∞‡§£:
    1. ‡§è‡§ï‡§æ ‡§ö‡§ø‡§§‡•ç‡§∞‡§™‡§ü‡§æ‡§ö‡•Ä ‡§§‡§ø‡§ï‡•Ä‡§ü ‡§ò‡•á‡§§‡§≤‡•Ä, ‡§™‡§£ ‡§ö‡§ø‡§§‡•ç‡§∞‡§™‡§ü ‡§Ü‡§µ‡§°‡§§ ‡§®‡§æ‡§π‡•Ä. ‡§§‡§∞‡•Ä‡§π‡•Ä "‡§™‡•à‡§∏‡•á ‡§µ‡§æ‡§Ø‡§æ ‡§ú‡§æ‡§§‡•Ä‡§≤" ‡§Æ‡•ç‡§π‡§£‡•Ç‡§® ‡§™‡•Ç‡§∞‡§æ ‡§ö‡§ø‡§§‡•ç‡§∞‡§™‡§ü ‡§¨‡§ò‡§£‡•á.
    2. ‡§ï‡•â‡§≤‡•á‡§ú‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§è‡§ï‡§æ ‡§µ‡§ø‡§∑‡§Ø‡§æ‡§§ ‡§ñ‡•Ç‡§™ ‡§µ‡•á‡§≥ ‡§ò‡§æ‡§≤‡§µ‡§≤‡§æ, ‡§™‡§£ ‡§Ü‡§§‡§æ ‡§§‡•ç‡§Ø‡§æ ‡§µ‡§ø‡§∑‡§Ø‡§æ‡§§ ‡§ï‡§∞‡§ø‡§Ö‡§∞ ‡§®‡§ï‡•ã. ‡§§‡§∞‡•Ä‡§π‡•Ä "‡§á‡§§‡§ï‡§æ ‡§µ‡•á‡§≥ ‡§µ‡§æ‡§Ø‡§æ ‡§ú‡§æ‡§à‡§≤" ‡§Æ‡•ç‡§π‡§£‡•Ç‡§® ‡§§‡•á‡§ö ‡§ï‡§∞‡§§ ‡§∞‡§æ‡§π‡§£‡•á.
    
    ‡§ü‡§æ‡§≥‡§£‡•ç‡§Ø‡§æ‡§ö‡•á ‡§Æ‡§æ‡§∞‡•ç‡§ó:
    - ‡§≠‡§µ‡§ø‡§∑‡•ç‡§Ø‡§æ‡§µ‡§∞ ‡§≤‡§ï‡•ç‡§∑ ‡§ï‡•á‡§Ç‡§¶‡•ç‡§∞‡§ø‡§§ ‡§ï‡§∞‡§æ, ‡§≠‡•Ç‡§§‡§ï‡§æ‡§≥‡§æ‡§µ‡§∞ ‡§®‡§æ‡§π‡•Ä
    - ‡§Ü‡§ß‡•Ä‡§ö‡•Ä ‡§ó‡•Å‡§Ç‡§§‡§µ‡§£‡•Ç‡§ï ‡§π‡§æ ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø‡§æ‡§ö‡§æ ‡§≠‡§æ‡§ó ‡§Æ‡§æ‡§®‡•Ç ‡§®‡§ï‡§æ
    - ‡§µ‡§∞‡•ç‡§§‡§Æ‡§æ‡§® ‡§™‡§∞‡§ø‡§∏‡•ç‡§•‡§ø‡§§‡•Ä‡§®‡•Å‡§∏‡§æ‡§∞ ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø ‡§ò‡•ç‡§Ø‡§æ
    """
    
    # Create temporary test file
    with tempfile.TemporaryDirectory() as temp_dir:
        test_file = os.path.join(temp_dir, "test_mental_models.txt")
        with open(test_file, 'w', encoding='utf-8') as f:
            f.write(test_data)
        
        # Test RAG chatbot (This would require actual Groq API key)
        print("üß™ Testing RAG Chatbot...")
        print("Note: This test requires a valid Groq API key in environment variable GROQ_API_KEY")
        
        groq_api_key = os.getenv("GROQ_API_KEY")
        if groq_api_key:
            try:
                # Initialize chatbot
                chatbot = RAGChatbot(
                    data_directory=temp_dir,
                    groq_api_key=groq_api_key
                )
                
                # Test question in Marathi
                test_question = "Sunk cost fallacy ‡§Ø‡§æ mental model ‡§≤‡§æ ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§ï‡§æ‡§Ø ‡§Æ‡•ç‡§π‡§£‡§æ‡§§‡§æ‡§§ ‡§Ü‡§£‡§ø ‡§§‡•ç‡§Ø‡§æ‡§ö‡•á ‡§â‡§¶‡§æ‡§π‡§∞‡§£ ‡§¶‡•ç‡§Ø‡§æ"
                print(f"\nüìù Test Question: {test_question}")
                
                response = chatbot.get_response(test_question)
                print(f"\nü§ñ Response: {response}")
                
                print("\n‚úÖ RAG Chatbot test completed successfully!")
                
            except Exception as e:
                print(f"‚ùå Error testing RAG chatbot: {e}")
        else:
            print("‚ö†Ô∏è GROQ_API_KEY not found in environment variables")
            print("üí° Set GROQ_API_KEY environment variable to test the chatbot")
        
        print("\n" + "="*50)
        print("RAG Module Test Summary:")
        print("- Document loading: ‚úÖ")
        print("- Vector store setup: ‚úÖ") 
        print("- Index creation: ‚úÖ")
        print("- Query engine: ‚úÖ")
        print("- Response generation: ‚úÖ (requires API key)")
        print("="*50)