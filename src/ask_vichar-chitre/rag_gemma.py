# import os
# import logging
# from typing import List, Optional
# from pathlib import Path

# from llama_index.core import VectorStoreIndex, Document, Settings, StorageContext
# from llama_index.core.node_parser import SentenceSplitter
# from llama_index.vector_stores.chroma import ChromaVectorStore
# from llama_index.embeddings.huggingface import HuggingFaceEmbedding
# from llama_index.llms.groq import Groq
# from llama_index.core.query_engine import RetrieverQueryEngine
# from llama_index.core.retrievers import VectorIndexRetriever
# from llama_index.core.postprocessor import SimilarityPostprocessor

# import chromadb
# from chromadb.config import Settings as ChromaSettings

# from dotenv import load_dotenv
# load_dotenv()

# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)

# # Model configs
# LLM_MODEL_AT_GROQ = "gemma2-9b-it"
# EMBEDDING_MODEL_AT_HUGGINGFACE = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
# # sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
# # ‚Üí This is a multilingual sentence embedding model, great for retrieving semantically similar text.
# # It supports multiple languages including Marathi, which is perfect for your use case.
# class RAGChatbot:
#     """
#     Retrieval-Augmented Generation Chatbot for Mental Models in Marathi
#     Uses LlamaIndex + ChromaDB + Groq (Gemma)
#     """

#     def __init__(self, data_directory: str, groq_api_key: str,
#                  model_name: str = LLM_MODEL_AT_GROQ,
#                  embedding_model: str = EMBEDDING_MODEL_AT_HUGGINGFACE):
#         self.data_directory = data_directory
#         self.groq_api_key = groq_api_key
#         self.model_name = model_name
#         self.embedding_model = embedding_model
#         self.db_path = "./chroma_db/"
#         self.collection_name = "mental_models_marathi"

#         # Initialization steps
#         print("üîß [1/6] Initializing LLM...")
#         self._setup_llm()
#         self._test_llm()

#         print("üîß [2/6] Loading embedding model...")
#         self._setup_embeddings()

#         print("üîß [3/6] Connecting to ChromaDB...")
#         self._setup_vector_store()

#         print("üîß [4/6] Loading documents...")
#         self._load_documents()

#         print("üîß [5/6] Creating index...")
#         self._create_index()

#         print("üîß [6/6] Setting up query engine...")
#         self._setup_query_engine()

#         logger.info("‚úÖ RAG Chatbot is ready to use!")

#     def _setup_llm(self):
#         try:
#             self.llm = Groq(
#                 model=self.model_name,
#                 api_key=self.groq_api_key,
#                 temperature=0.3,
#                 max_tokens=1024
#             )
#             Settings.llm = self.llm
#         except Exception as e:
#             logger.error(f"‚ùå LLM Setup Failed: {e}")
#             raise

#     def _test_llm(self):
#         try:
#             print("\nüß™ Running LLM self-test:")
#             prompts = [
#                 ("EN", "What is a mental model? Answer in one sentence."),
#                 ("MR", "Mental model ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§ï‡§æ‡§Ø? ‡§è‡§ï‡§æ ‡§µ‡§æ‡§ï‡•ç‡§Ø‡§æ‡§§ ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•ç‡§Ø‡§æ."),
#                 ("MR", "Inversion ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§ï‡§æ‡§Ø?")
#             ]
#             for lang, prompt in prompts:
#                 print(f"\nüß† {lang} Prompt ‚Üí {prompt}")
#                 res = self.llm.complete(prompt)
#                 print(f"üîÅ Response ‚Üí {res}")
#         except Exception as e:
#             print(f"‚ùå LLM Self-Test Failed: {e}")
#             raise

#     def _setup_embeddings(self):
#         try:
#             self.embed_model = HuggingFaceEmbedding(
#                 model_name=self.embedding_model,
#                 cache_folder="./embeddings_cache"
#             )
#             Settings.embed_model = self.embed_model
#         except Exception as e:
#             logger.error(f"‚ùå Embedding Setup Failed: {e}")
#             raise

#     def _setup_vector_store(self):
#         try:
#             os.makedirs(self.db_path, exist_ok=True)
#             chroma_client = chromadb.PersistentClient(
#                 path=self.db_path,
#                 settings=ChromaSettings(anonymized_telemetry=False)
#             )
#             try:
#                 chroma_collection = chroma_client.get_collection(self.collection_name)
#             except:
#                 chroma_collection = chroma_client.create_collection(self.collection_name)

#             self.vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
#             self.storage_context = StorageContext.from_defaults(vector_store=self.vector_store)
#         except Exception as e:
#             logger.error(f"‚ùå Vector Store Setup Failed: {e}")
#             raise

#     def _load_documents(self):
#         try:
#             self.documents = []
#             path = Path(self.data_directory)
#             if not path.exists():
#                 raise FileNotFoundError(f"{self.data_directory} does not exist")

#             for file_path in path.rglob('*'):
#                 if file_path.suffix.lower() in [".txt", ".md", ".tex", ".json"]:
#                     with open(file_path, 'r', encoding='utf-8') as f:
#                         content = f.read().strip()
#                         if content:
#                             self.documents.append(Document(
#                                 text=content,
#                                 metadata={
#                                     "filename": file_path.name,
#                                     "filepath": str(file_path),
#                                     "file_type": file_path.suffix
#                                 }
#                             ))
#                             print(f"üìÑ Loaded: {file_path.name} ({len(content)} chars)")
#             if not self.documents:
#                 raise ValueError("No valid documents found.")
#         except Exception as e:
#             logger.error(f"‚ùå Document Load Failed: {e}")
#             raise

#     def _create_index(self):
#         try:
#             if self.vector_store.client.count() > 0:
#                 self.index = VectorStoreIndex.from_vector_store(self.vector_store)
#                 logger.info("‚úÖ Loaded existing index.")
#                 return

#             Settings.text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)
#             self.index = VectorStoreIndex.from_documents(
#                 self.documents,
#                 storage_context=self.storage_context,
#                 show_progress=True
#             )
#             logger.info("‚úÖ Index created.")
#         except Exception as e:
#             logger.error(f"‚ùå Index Creation Failed: {e}")
#             raise

#     def _setup_query_engine(self):
#         try:
#             retriever = VectorIndexRetriever(index=self.index, similarity_top_k=3)
#             self.query_engine = RetrieverQueryEngine(
#                 retriever=retriever,
#                 node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)]
#             )
#         except Exception as e:
#             logger.error(f"‚ùå Query Engine Setup Failed: {e}")
#             raise

#     def diagnose_query_engine(self, question: str) -> str:
#         print(f"\nüß† User Question: {question}")
#         print("-" * 80)

#         # Step 1: Retrieve
#         print("üîé Step 1: Retrieving relevant nodes...")
#         retriever = VectorIndexRetriever(index=self.index, similarity_top_k=3)
#         nodes = retriever.retrieve(question)
#         for i, node in enumerate(nodes):
#             print(f"\nüîπ Node {i+1} (score={node.score:.2f}):\n{node.text[:200]}...\n")

#         # Step 2: Query engine
#         print("üß† Step 2: Getting response from query engine...")
#         response = self.query_engine.query(question)
#         print(f"\n‚úÖ Engine Response:\n{str(response)}")

#         # Step 3: Direct LLM with context
#         print("\nü§ñ Step 3: Direct LLM using top context...")
#         if nodes:
#             context = "\n\n".join([node.text for node in nodes[:2]])
#             prompt = f"""Context:
# {context}

# Question: {question}

# Answer in Marathi if question is in Marathi."""
#             llm_response = self.llm.complete(prompt)
#             print(f"\nüéØ Final Answer:\n{llm_response}")
#             return str(llm_response)

#         return str(response)

#     # def get_response(self, question: str) -> str:
#     #     return self.diagnose_query_engine(question)
#     def get_response(self, query: str) -> dict:
#             try:
#                 # Step 1: retrieve top nodes
#                 retriever = VectorIndexRetriever(index=self.index, similarity_top_k=3)
#                 nodes = retriever.retrieve(query)

#                 # Step 2: generate final answer
#                 response = self.query_engine.query(query)

#                 # Step 3: prepare context text
#                 context = "\n\n".join([node.text for node in nodes])

#                 return {
#                     "answer": str(response),
#                     "context": context
#                 }
#             except Exception as e:
#                 return {
#                     "answer": f"‚ùå Error while generating answer: {e}",
#                     "context": ""
#                 }

# if __name__ == "__main__":
#     print("\nüöÄ Starting Diagnostic Run for RAG Chatbot...\n")
#     groq_api_key = os.getenv("GROQ_API_KEY")

#     if not groq_api_key:
#         print("‚ùå GROQ_API_KEY not found. Please set it in your .env file or environment variables.")
#     else:
#         try:
#             chatbot = RAGChatbot(
#                 data_directory="data",
#                 groq_api_key=groq_api_key
#             )
#             print("\n" + "=" * 80)
#             print("üß™ TEST: Diagnostic for query 'Mental model ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§ï‡§æ‡§Ø?'")
#             print("=" * 80)
#             chatbot.get_response("Mental model ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§ï‡§æ‡§Ø?")
#         except Exception as e:
#             print(f"\n‚ùå Exception during chatbot run: {e}")
# import os
# import torch
# from dotenv import load_dotenv
# from transformers import AutoTokenizer, AutoModelForCausalLM
# from accelerate import init_empty_weights, disk_offload
# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings
# from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# class RAGChatbot:
#     def __init__(self, data_directory: str):
#         load_dotenv()

#         # Step 1: Load documents
#         reader = SimpleDirectoryReader(input_dir=data_directory, recursive=True)
#         documents = reader.load_data()
#         print("üìÑ Documents loaded:", [doc.metadata.get("file_path") for doc in documents])

#         # Step 2: Embedding model
#         embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
#         Settings.embed_model = embed_model

#         # Step 3: Vector index and retriever
#         self.index = VectorStoreIndex.from_documents(documents)
#         self.retriever = self.index.as_retriever()
#         print("‚úÖ Vector index and retriever initialized.")

#         # Step 4: Load tokenizer
#         print("üîÅ Loading the Shivneri tokenizer...")
#         self.tokenizer = AutoTokenizer.from_pretrained("amitagh/shivneri-marathi-llm-7b-v0.1")

#         # Step 5: Disk offload model loading (manual)
#         print("ü™∂ Initializing empty model for disk offload...")
#         offload_dir = "offload_folder"
#         os.makedirs(offload_dir, exist_ok=True)

#         with init_empty_weights():
#             empty_model = AutoModelForCausalLM.from_pretrained("amitagh/shivneri-marathi-llm-7b-v0.1")

#         print("üíæ Offloading model to disk...")
#         self.model = disk_offload(empty_model, offload_dir=offload_dir)

#         print("üöÄ Shivneri model loaded with disk offload.")

#     def get_response(self, query: str) -> dict:
#         nodes = self.retriever.retrieve(query)
#         context_text = "\n".join([n.text for n in nodes])

#         prompt = (
#             "‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•ç‡§Ø‡§æ‡§Ø‡§ö‡§Ç ‡§Ü‡§π‡•á. ‡§ñ‡§æ‡§≤‡•Ä ‡§¶‡§ø‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä‡§ö‡•ç‡§Ø‡§æ ‡§Ü‡§ß‡§æ‡§∞‡•á ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§æ‡§ö‡•á ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•ç‡§Ø‡§æ:\n\n"
#             f"‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä: {context_text}\n\n"
#             f"‡§™‡•ç‡§∞‡§∂‡•ç‡§®: {query}\n"
#             "‡§â‡§§‡•ç‡§§‡§∞:"
#         )

#         inputs = self.tokenizer(prompt, return_tensors="pt")
#         with torch.no_grad():
#             outputs = self.model.generate(
#                 **inputs,
#                 max_new_tokens=200,
#                 temperature=0.7,
#                 do_sample=True,
#                 top_p=0.9
#             )
#         full_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
#         answer = full_output.split("‡§â‡§§‡•ç‡§§‡§∞:")[-1].strip()

#         return {
#             "answer": answer,
#             "context": context_text.strip()
#         }

# # ‚úÖ Run test
# if __name__ == "__main__":
#     print("üõÅ Done with your shower? Let's test the chatbot.")
#     bot = RAGChatbot(data_directory="data")
#     query = "Inversion ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§ï‡§æ‡§Ø?"
#     result = bot.get_response(query)

#     print("\nüß† ‡§â‡§§‡•ç‡§§‡§∞:", result["answer"])
#     print("\nüîç ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä:\n", result["context"][:500])
import os
import logging
from typing import List
from pathlib import Path

from llama_index.core import VectorStoreIndex, Document, Settings, StorageContext
from llama_index.core.node_parser import SentenceSplitter
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.groq import Groq
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.postprocessor import SimilarityPostprocessor

import chromadb
from chromadb.config import Settings as ChromaSettings
from dotenv import load_dotenv

load_dotenv()
groq_api_key = os.getenv("GROQ_API_KEY")
# Logging setup
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Config
LLM_MODEL_AT_GROQ = "gemma2-9b-it"
EMBEDDING_MODEL_AT_HUGGINGFACE = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

class RAGChatbot:
    def __init__(self, data_directory: str, groq_api_key: str,
                 model_name: str = LLM_MODEL_AT_GROQ,
                 embedding_model: str = EMBEDDING_MODEL_AT_HUGGINGFACE):
        self.data_directory = data_directory
        self.groq_api_key = groq_api_key
        self.model_name = model_name
        self.embedding_model = embedding_model
        self.db_path = "./chroma_db/"
        self.collection_name = "mental_models_marathi"

        print("üîß [1/6] Initializing LLM...")
        self._setup_llm()
        self._test_llm()

        print("üîß [2/6] Loading embedding model...")
        self._setup_embeddings()

        print("üîß [3/6] Connecting to ChromaDB...")
        self._setup_vector_store()

        print("üîß [4/6] Loading documents...")
        self._load_documents()

        print("üîß [5/6] Creating index...")
        self._create_index()

        print("üîß [6/6] Setting up query engine...")
        self._setup_query_engine()

        logger.info("‚úÖ RAG Chatbot is ready to use!")

    def _setup_llm(self):
        try:
            self.llm = Groq(
                model=self.model_name,
                api_key=self.groq_api_key,
                temperature=0.3,
                max_tokens=1024
            )
            Settings.llm = self.llm
        except Exception as e:
            logger.error(f"‚ùå LLM Setup Failed: {e}")
            raise

    def _test_llm(self):
        try:
            print("\nüß™ LLM Self-test:")
            prompts = [
                "Mental model ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§ï‡§æ‡§Ø?",
                "Inversion ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§ï‡§æ‡§Ø?",
                "Explain mental models in 1 sentence."
            ]
            for prompt in prompts:
                print(f"\nüîπ Prompt: {prompt}")
                result = self.llm.complete(prompt)
                print(f"üî∏ Response: {result}")
        except Exception as e:
            logger.error(f"‚ùå LLM self-test failed: {e}")
            raise

    def _setup_embeddings(self):
        try:
            self.embed_model = HuggingFaceEmbedding(
                model_name=self.embedding_model,
                cache_folder="./embeddings_cache"
            )
            Settings.embed_model = self.embed_model
        except Exception as e:
            logger.error(f"‚ùå Embedding model load failed: {e}")
            raise

    def _setup_vector_store(self):
        try:
            os.makedirs(self.db_path, exist_ok=True)
            chroma_client = chromadb.PersistentClient(
                path=self.db_path,
                settings=ChromaSettings(anonymized_telemetry=False)
            )

            try:
                chroma_collection = chroma_client.get_collection(self.collection_name)
            except:
                chroma_collection = chroma_client.create_collection(self.collection_name)

            self.vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
            self.storage_context = StorageContext.from_defaults(vector_store=self.vector_store)
        except Exception as e:
            logger.error(f"‚ùå ChromaDB setup failed: {e}")
            raise

    def _load_documents(self):
        try:
            self.documents = []
            path = Path(self.data_directory)
            if not path.exists():
                raise FileNotFoundError(f"{self.data_directory} not found")

            for file_path in path.rglob("*"):
                if file_path.suffix.lower() in [".txt", ".md", ".tex"]:
                    with open(file_path, "r", encoding="utf-8") as f:
                        content = f.read().strip()
                        if content:
                            self.documents.append(Document(
                                text=content,
                                metadata={"filename": file_path.name}
                            ))
                            print(f"üìÑ Loaded: {file_path.name}")
            if not self.documents:
                raise ValueError("‚ùå No documents found!")
        except Exception as e:
            logger.error(f"‚ùå Document loading failed: {e}")
            raise

    def _create_index(self):
        try:
            if self.vector_store.client.count() > 0:
                self.index = VectorStoreIndex.from_vector_store(self.vector_store)
                logger.info("‚úÖ Loaded existing index.")
            else:
                Settings.text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)
                self.index = VectorStoreIndex.from_documents(
                    self.documents,
                    storage_context=self.storage_context,
                    show_progress=True
                )
                logger.info("‚úÖ Created new index.")
        except Exception as e:
            logger.error(f"‚ùå Index creation failed: {e}")
            raise

    def _setup_query_engine(self):
        try:
            retriever = VectorIndexRetriever(index=self.index, similarity_top_k=3)
            self.query_engine = RetrieverQueryEngine(
                retriever=retriever,
                node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)]
            )
        except Exception as e:
            logger.error(f"‚ùå Query engine setup failed: {e}")
            raise

    def get_response(self, query: str) -> dict:
        try:
            retriever = VectorIndexRetriever(index=self.index, similarity_top_k=3)
            nodes = retriever.retrieve(query)
            response = self.query_engine.query(query)
            context = "\n\n".join([node.text for node in nodes])
            return {
                "answer": str(response).strip(),
                "context": context.strip()
            }
        except Exception as e:
            return {
                "answer": f"‚ùå Error while generating answer: {e}",
                "context": ""
            }

# Optional CLI test
if __name__ == "__main__":
    groq_api_key = os.getenv("GROQ_API_KEY")
    if not groq_api_key:
        print("‚ùå GROQ_API_KEY missing. Set it in .env")
    else:
        bot = RAGChatbot(data_directory="data", groq_api_key=groq_api_key)
        result = bot.get_response("Inversion ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§ï‡§æ‡§Ø?")
        print("\nüß† ‡§â‡§§‡•ç‡§§‡§∞:", result["answer"])
        print("\nüìé ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠:\n", result["context"][:500])
