# import streamlit as st
# import torch
# from transformers import AutoTokenizer, pipeline
# from peft import AutoPeftModelForCausalLM
# import re

# # ======================
# # 1. Load Model + Tokenizer
# # ======================
# MODEL_PATH = "./gemma3-marathi-lora"  # your fine-tuned LoRA folder

# @st.cache_resource
# def load_model():
#     model = AutoPeftModelForCausalLM.from_pretrained(
#         MODEL_PATH,
#         torch_dtype=torch.float32,
#         device_map=None  # CPU only
#     )
#     tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

#     pipe = pipeline(
#         "text-generation",
#         model=model,
#         tokenizer=tokenizer,
#         torch_dtype=torch.float32,
#         device=-1
#     )
#     return pipe, tokenizer

# pipe, tokenizer = load_model()

# # ======================
# # 2. Streamlit UI
# # ======================
# st.set_page_config(page_title="Marathi Chatbot", page_icon="ü™î", layout="centered")
# st.title("ü™î Marathi Chatbot (Gemma3 LoRA)")

# # Initialize chat history
# if "messages" not in st.session_state:
#     st.session_state.messages = []

# # Display conversation
# for msg in st.session_state.messages:
#     role = "üßë‚Äçüíª You" if msg["role"] == "user" else "ü§ñ Bot"
#     st.markdown(f"**{role}:** {msg['content']}")

# # Input box
# user_input = st.text_area("‚úçÔ∏è ‡§§‡•Å‡§Æ‡§ö‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§≤‡§ø‡§π‡§æ:", "")

# # Handle button click
# if st.button("‡§â‡§§‡•ç‡§§‡§∞ ‡§Æ‡§ø‡§≥‡§µ‡§æ"):
#     if user_input.strip():
#         # Save user message
#         st.session_state.messages.append({"role": "user", "content": user_input})

#         with st.spinner("ü§î ‡§µ‡§ø‡§ö‡§æ‡§∞ ‡§ö‡§æ‡§≤‡•Ç ‡§Ü‡§π‡•á..."):
#             # Instruction to force Marathi output
#             final_prompt = (
#                 "‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§è‡§ï ‡§π‡•Å‡§∂‡§æ‡§∞ ‡§∏‡§π‡§æ‡§Ø‡•ç‡§Ø‡§ï ‡§Ü‡§π‡§æ‡§§ ‡§ú‡•ã ‡§®‡•á‡§π‡§Æ‡•Ä ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§, ‡§∏‡•ã‡§™‡•ç‡§Ø‡§æ ‡§Ü‡§£‡§ø ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§≠‡§æ‡§∑‡•á‡§§ ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§§‡•ã.\n\n"
#                 "‡§â‡§¶‡§æ‡§π‡§∞‡§£:\n"
#                 "‡§™‡•ç‡§∞‡§∂‡•ç‡§®: Lindy Effect ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§ï‡§æ‡§Ø?\n"
#                 "‡§â‡§§‡•ç‡§§‡§∞: ‡§≤‡§ø‡§Ç‡§°‡•Ä ‡§á‡§´‡•á‡§ï‡•ç‡§ü ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§è‡§ñ‡§æ‡§¶‡•Ä ‡§ó‡•ã‡§∑‡•ç‡§ü ‡§ú‡§ø‡§§‡§ï‡•Ä ‡§ú‡•Å‡§®‡•Ä ‡§Ö‡§∏‡•á‡§≤ ‡§§‡§ø‡§§‡§ï‡•Ä ‡§§‡•Ä ‡§™‡•Å‡§¢‡•á ‡§ü‡§ø‡§ï‡§£‡•ç‡§Ø‡§æ‡§ö‡•Ä ‡§∂‡§ï‡•ç‡§Ø‡§§‡§æ ‡§ú‡§æ‡§∏‡•ç‡§§ ‡§Ö‡§∏‡§§‡•á.\n\n"
#                 f"‡§™‡•ç‡§∞‡§∂‡•ç‡§®: {user_input}\n‡§â‡§§‡•ç‡§§‡§∞:"
#             )


#             output = pipe(
#                 final_prompt,
#                 max_new_tokens=200,
#                 do_sample=True,
#                 temperature=0.6,   # slightly lower = more focused
#                 top_p=0.85,
#                 repetition_penalty=1.5,
#                 eos_token_id=tokenizer.eos_token_id
#             )

#             # Extract and clean response
#             response = output[0]["generated_text"].replace(final_prompt, "").strip()
#             response = re.sub(r"<.*?>", "", response)   # remove HTML tags
#             response = re.sub(r"[a-zA-Z]+", "", response)  # drop stray English
#             response = re.sub(r"\s+", " ", response).strip()  # clean spaces

#         # Save bot message
#         st.session_state.messages.append({"role": "bot", "content": response})

#         # Refresh chat
#         st.rerun()

#//generatting random ques

# import streamlit as st
# import torch
# from transformers import AutoTokenizer, pipeline
# from peft import AutoPeftModelForCausalLM
# import re

# # ======================
# # 1. Load Model + Tokenizer
# # ======================
# MODEL_PATH = "./gemma3-marathi-lora"  # your fine-tuned LoRA folder

# @st.cache_resource
# def load_model():
#     model = AutoPeftModelForCausalLM.from_pretrained(
#         MODEL_PATH,
#         torch_dtype=torch.float32,
#         device_map=None  # CPU only
#     )
#     tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

#     pipe = pipeline(
#         "text-generation",
#         model=model,
#         tokenizer=tokenizer,
#         torch_dtype=torch.float32,
#         device=-1
#     )
#     return pipe, tokenizer

# pipe, tokenizer = load_model()

# # ======================
# # 2. Streamlit UI
# # ======================
# st.set_page_config(page_title="Marathi Chatbot", page_icon="ü™î", layout="centered")
# st.title("Marathi Chatbot (Gemma3 LoRA)")

# # Initialize chat history
# if "messages" not in st.session_state:
#     st.session_state.messages = []

# # Display conversation
# for msg in st.session_state.messages:
#     role = "üßë‚Äçüíª You" if msg["role"] == "user" else "ü§ñ Bot"
#     st.markdown(f"**{role}:** {msg['content']}")

# # Input box
# user_input = st.text_area("‚úçÔ∏è ‡§§‡•Å‡§Æ‡§ö‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§≤‡§ø‡§π‡§æ:", "")

# # Handle button click
# if st.button("‡§â‡§§‡•ç‡§§‡§∞ ‡§Æ‡§ø‡§≥‡§µ‡§æ"):
#     if user_input.strip():
#         # Save user message
#         st.session_state.messages.append({"role": "user", "content": user_input})

#         with st.spinner("ü§î ‡§µ‡§ø‡§ö‡§æ‡§∞ ‡§ö‡§æ‡§≤‡•Ç ‡§Ü‡§π‡•á..."):
#             # Match the fine-tuned training format
#             final_prompt = (
#                 f"‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§è‡§ï ‡§â‡§™‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§∏‡§π‡§æ‡§Ø‡•ç‡§Ø‡§ï ‡§Ü‡§π‡§æ‡§§ ‡§ú‡•ã ‡§®‡•á‡§π‡§Æ‡•Ä ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§, ‡§•‡•ã‡§°‡§ï‡•ç‡§Ø‡§æ‡§§ ‡§Ü‡§£‡§ø ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü‡§™‡§£‡•á ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§§‡•ã.\n\n"
#                 f"‡§™‡•ç‡§∞‡§∂‡•ç‡§®: {user_input}\n"
#                 f"‡§â‡§§‡•ç‡§§‡§∞ (‡§•‡•ã‡§°‡§ï‡•ç‡§Ø‡§æ‡§§, ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§Ö‡§∞‡•ç‡§•):"
#             )

#             output = pipe(
#                 final_prompt,
#                 max_new_tokens=200,
#                 do_sample=True,
#                 temperature=0.6,   # lower = more focused
#                 top_p=0.9,
#                 repetition_penalty=1.3,
#                 eos_token_id=tokenizer.eos_token_id
#             )

#             # Extract and clean response
#             generated_text = output[0]["generated_text"]
#             response = generated_text[len(final_prompt):].strip()  # cut prompt part
#             response = re.sub(r"<.*?>", "", response)  # remove HTML tags
#             response = re.sub(r"\s+", " ", response).strip()  # clean spaces

#         # Save bot message
#         st.session_state.messages.append({"role": "bot", "content": response})

#         # Refresh chat
#         st.rerun()

#upoad file
# import streamlit as st
# import torch
# from transformers import AutoTokenizer, pipeline
# from peft import AutoPeftModelForCausalLM
# import re

# MODEL_PATH = "./gemma3-marathi-lora"  # your fine-tuned LoRA

# @st.cache_resource
# def load_model():
#     model = AutoPeftModelForCausalLM.from_pretrained(
#         MODEL_PATH,
#         torch_dtype=torch.float32,
#         device_map=None
#     )
#     tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
#     pipe = pipeline(
#         "text-generation",
#         model=model,
#         tokenizer=tokenizer,
#         torch_dtype=torch.float32,
#         device=-1  # CPU
#     )
#     return pipe, tokenizer

# pipe, tokenizer = load_model()

# st.title("üìñ Marathi Q&A from .tex files")

# # Upload .tex
# uploaded_file = st.file_uploader("Upload .tex file", type=["tex"])

# def clean_tex(tex_text):
#     text = re.sub(r"\\[a-zA-Z]+\{.*?\}", "", tex_text)
#     text = re.sub(r"\\[a-zA-Z]+", "", text)
#     text = re.sub(r"\{|\}", "", text)
#     return text.strip()

# context = ""
# if uploaded_file:
#     tex_text = uploaded_file.read().decode("utf-8")
#     context = clean_tex(tex_text)
#     st.text_area("Extracted Marathi Text", context, height=250)

# # Ask Question
# question = st.text_input("‡§§‡•Å‡§Æ‡§ö‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§µ‡§ø‡§ö‡§æ‡§∞‡§æ (in Marathi):")

# if question and context:
#     with st.spinner("ü§î ‡§µ‡§ø‡§ö‡§æ‡§∞ ‡§ö‡§æ‡§≤‡•Ç ‡§Ü‡§π‡•á..."):
#         final_prompt = (
#             "‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§è‡§ï ‡§π‡•Å‡§∂‡§æ‡§∞ ‡§∏‡§π‡§æ‡§Ø‡•ç‡§Ø‡§ï ‡§Ü‡§π‡§æ‡§§ ‡§ú‡•ã ‡§®‡•á‡§π‡§Æ‡•Ä ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§, ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§≠‡§æ‡§∑‡•á‡§§ ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§§‡•ã.\n\n"
#             f"‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§Æ‡§ú‡§ï‡•Ç‡§∞:\n{context}\n\n"
#             f"‡§™‡•ç‡§∞‡§∂‡•ç‡§®: {question}\n‡§â‡§§‡•ç‡§§‡§∞:"
#         )

#         output = pipe(
#             final_prompt,
#             max_new_tokens=200,
#             do_sample=True,
#             temperature=0.6,
#             top_p=0.9,
#             repetition_penalty=1.3,
#             eos_token_id=tokenizer.eos_token_id
#         )

#         response = output[0]["generated_text"].replace(final_prompt, "").strip()
#         response = re.sub(r"<.*?>", "", response)
#         response = re.sub(r"\s+", " ", response).strip()

#     st.success("üìñ ‡§â‡§§‡•ç‡§§‡§∞:")
#     st.write(response)
import streamlit as st
import torch
from transformers import AutoTokenizer, pipeline
from peft import AutoPeftModelForCausalLM
import re

MODEL_PATH = "./gemma3-marathi-lora"  # your fine-tuned LoRA

@st.cache_resource
def load_model():
    model = AutoPeftModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.float32,
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        torch_dtype=torch.float32
    )
    return pipe, tokenizer

pipe, tokenizer = load_model()

st.title("üìñ Marathi Q&A from .tex files")

# Upload .tex
uploaded_file = st.file_uploader("Upload .tex file", type=["tex"])

def clean_tex(tex_text):
    text = re.sub(r"\\[a-zA-Z]+\{.*?\}", "", tex_text)   # remove LaTeX commands
    text = re.sub(r"\\[a-zA-Z]+", "", text)
    text = re.sub(r"\{|\}", "", text)
    return text.strip()

context = ""
if uploaded_file:
    tex_text = uploaded_file.read().decode("utf-8")
    context = clean_tex(tex_text)
    st.text_area("Extracted Marathi Text", context, height=250)

# Ask Question
question = st.text_input("‡§§‡•Å‡§Æ‡§ö‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§µ‡§ø‡§ö‡§æ‡§∞‡§æ (in Marathi):")

if question and context:
    with st.spinner("ü§î ‡§µ‡§ø‡§ö‡§æ‡§∞ ‡§ö‡§æ‡§≤‡•Ç ‡§Ü‡§π‡•á..."):
        # Force Marathi output by explicitly instructing the model
        final_prompt = (
            f"‡§´‡§ï‡•ç‡§§ ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•ç‡§Ø‡§æ.\n"
            f"‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠‡§æ‡§µ‡§∞ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§æ‡§ö‡•á ‡§â‡§§‡•ç‡§§‡§∞ ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§¶‡•ç‡§Ø‡§æ.\n\n"
            f"‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠:\n{context}\n\n"
            f"‡§™‡•ç‡§∞‡§∂‡•ç‡§®: {question}\n\n"
            f"‡§â‡§§‡•ç‡§§‡§∞:"
        )

        output = pipe(
            final_prompt,
            max_new_tokens=300,
            do_sample=True,
            temperature=0.6,
            top_p=0.9,
            repetition_penalty=1.2,
            eos_token_id=tokenizer.eos_token_id
        )

        response = output[0]["generated_text"]
        # Remove the prompt part
        response = response.replace(final_prompt, "").strip()
        # Remove any unwanted tokens or HTML
        response = re.sub(r"<.*?>", "", response)
        response = re.sub(r"\s+", " ", response).strip()

    st.success("üìñ ‡§â‡§§‡•ç‡§§‡§∞:")
    st.write(response)
