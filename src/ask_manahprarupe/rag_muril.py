import os
import torch
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModel
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.llms import MockLLM
from llama_index.core.query_engine import RetrieverQueryEngine
from chromadb import PersistentClient

# Load environment variables
load_dotenv()
hf_token = os.getenv("HUGGINGFACE_TOKEN")

if not hf_token:
    raise ValueError("âŒ Hugging Face token not found in .env file. Please add HUGGINGFACE_TOKEN to your .env.")

class RAGChatbot:
    def __init__(self, data_directory: str):
        print("ğŸ§  Loading documents...")
        documents = SimpleDirectoryReader(data_directory).load_data()

        print("ğŸ” Splitting into chunks...")
        parser = SentenceSplitter(chunk_size=256, chunk_overlap=30)

        print("ğŸ“Œ Setting up embedding model (MURIL)...")
        embed_model = HuggingFaceEmbedding(
            model_name="google/muril-base-cased",
            device="cuda" if torch.cuda.is_available() else "cpu",
            max_length=512
        )

        Settings.embed_model = embed_model
        Settings.node_parser = parser
        Settings.llm = MockLLM()

        print("ğŸ“š Setting up Chroma vector store...")
        persist_directory = "./chroma_db"
        os.makedirs(persist_directory, exist_ok=True)

        try:
            chroma_client = PersistentClient(path=persist_directory)
            chroma_client.delete_collection("muril_docs")
        except Exception as e:
            print(f"âš ï¸ Collection deletion warning: {e}")

        chroma_client = PersistentClient(path=persist_directory)
        chroma_collection = chroma_client.get_or_create_collection("muril_docs")

        vector_store = ChromaVectorStore(
            chroma_collection=chroma_collection,
            client=chroma_client
        )

        print("âš™ï¸ Creating vector index...")
        nodes = parser.get_nodes_from_documents(documents)
        index = VectorStoreIndex(nodes, vector_store=vector_store)

        retriever = index.as_retriever(similarity_top_k=3)
        retriever.similarity_cutoff = 0.65

        self.query_engine = RetrieverQueryEngine.from_args(retriever=retriever)
        print("âœ… RAG setup complete.")

    def extract_answer_from_context(self, context_nodes, query):
        if not context_nodes:
            return "âŒ à¤¸à¤‚à¤¦à¤°à¥à¤­ à¤¸à¤¾à¤ªà¤¡à¤²à¥‡ à¤¨à¤¾à¤¹à¥€à¤¤."

        print("\nğŸ§  Retrieved Nodes:")
        for i, node in enumerate(context_nodes):
            print(f"\nğŸ”¹ Node {i+1} (Score: {node.score:.3f}):")
            print(node.text.strip()[:300] + "...\n")

        best_node = max(context_nodes, key=lambda x: x.score)
        answer = best_node.text.strip()

        for line in answer.split("\n"):
            if any(word in line for word in query.split()):
                return line.strip()

        return answer if answer else "à¤®à¤¾à¤« à¤•à¤°à¤¾, à¤®à¤²à¤¾ à¤¯à¤¾ à¤ªà¥à¤°à¤¶à¥à¤¨à¤¾à¤šà¥‡ à¤‰à¤¤à¥à¤¤à¤° à¤¸à¤¾à¤ªà¤¡à¤²à¥‡ à¤¨à¤¾à¤¹à¥€."

    def chat(self, query: str) -> str:
        print(f"\nğŸ§¾ User: {query}")
        response = self.query_engine.query(query)
        context_nodes = response.source_nodes
        answer = self.extract_answer_from_context(context_nodes, query)
        return answer

if __name__ == "__main__":
    print("ğŸ› Done with your shower? Let's test the chatbot.")
    bot = RAGChatbot(data_directory="data")

    while True:
        user_input = input("\nYou: ")
        if user_input.lower() in ["exit", "quit"]:
            break
        response = bot.chat(user_input)
        print("ğŸ¤– Shivneri Bot:", response)
