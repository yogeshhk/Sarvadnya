{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pip3-autoremove\n",
    "!pip-autoremove torch torchvision torchaudio -y\n",
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install unsloth\n",
    "!pip install --no-deps git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3\n",
    "!pip install pytorch-lightning\n",
    "#!pip install accelerate\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "#     for i in range(torch.cuda.device_count()):\n",
    "#         print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "# else:\n",
    "#     print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.18: Fast Gemma3 patching. Transformers: 4.50.0.dev0.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.168 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9a9d4da3214837a1e9079ef4c3bbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4b87cbbec64777a453a88ad5e7157a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db879de51ee450ebe42a8f2d75944ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "#import torchvision\n",
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    full_finetuning = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.vision_tower.vision_model.encoder` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 34,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    # finetune_vision_layers = False,\n",
    "    # finetune_language_layers = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mar = pd.read_csv(\"/teamspace/studios/this_studio/English-to-Marathi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# Combine English and Marathi columns into a 'text' column\n",
    "df_mar[\"text\"] = df_mar[\"English\"] + \" || \" + df_mar[\"Marathi\"]\n",
    "\n",
    "# Convert the pandas DataFrame into a Hugging Face Dataset\n",
    "dataset_mar = Dataset.from_pandas(df_mar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas DataFrame into a Hugging Face Dataset\n",
    "custom_prompt = \"\"\"Below is a document containing English text and its Marathi translation. \n",
    "                   Your task is to understand the language complexity, and learn the marathi \n",
    "                   semantic content for future translation from English to Marathi language fluently.\n",
    "\n",
    "### English:\n",
    "{}\n",
    "\n",
    "### Marathi:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  \n",
    "\n",
    "# Function to format the prompts\n",
    "def formatting_prompts_func(examples):\n",
    "    english_texts = examples[\"English\"]\n",
    "    marathi_texts = examples[\"Marathi\"]\n",
    "    outputs = [\"\"] * len(english_texts) \n",
    "\n",
    "    formatted_texts = []\n",
    "    for english_text, marathi_text, output in zip(english_texts, marathi_texts, outputs):\n",
    "        formatted_text = custom_prompt.format(english_text, marathi_text, output) + EOS_TOKEN \n",
    "        formatted_texts.append(formatted_text)\n",
    "\n",
    "    return {\"text\": formatted_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7243437cecf4ad1bb1887b26af7288d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3636247 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Apply the `map` function to the subset\n",
    "dataset_mar = dataset_mar.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad56b2c5f3c8442988a381a65a52ec7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/3636247 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move to GPU or CPU as required\n",
    "\n",
    "# Define trainer with consistent precision and multi-GPU support\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  \n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset_mar,  # Processed dataset\n",
    "    dataset_text_field=\"text\",  # Combined 'text' column\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=1,\n",
    "    packing=False,  # Can make training faster fjor short sequences\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=8,  # Adjust for memory constraints\n",
    "        gradient_accumulation_steps=4,  # Manage gradients effectively\n",
    "        num_train_epochs=3,  # Number of training epochs\n",
    "        warmup_steps=10,\n",
    "        max_steps=500,  \n",
    "        learning_rate=2e-6,  # Optimized learning rate, adjust as needed\n",
    "        fp16=not is_bfloat16_supported(),  \n",
    "        bf16=is_bfloat16_supported(),  \n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",  # Memory-efficient optimizer\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=2802,\n",
    "        output_dir=\"outputs\",  # Output directory\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 3,636,247 | Num Epochs = 1 | Total steps = 500\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 273,825,792/12,000,000,000 (2.28% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 1:22:46, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.440200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.366600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.257100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.212900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.858700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.573100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.426500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.288600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.135500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>3.015400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.876000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.762200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.689900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.536400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.407300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.340500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.169600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.121200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.961000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.848500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.785700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.682900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.531500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.479700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.399700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.378300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.314500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.286200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.234600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.189400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.174900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.162700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.175100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.208400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.172500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.190300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.149900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.141200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.130500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.110600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.172200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.097600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.137600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.189800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.163500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.130700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "peshwa_df = pd.read_csv(\"/teamspace/studios/this_studio/About-Peshwaii-Complete-Marathi-text.txt\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas DataFrame into a Hugging Face Dataset\n",
    "dataset_peshwe = Dataset.from_pandas(peshwa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas DataFrame into a Hugging Face Dataset\n",
    "custom_prompt = \"\"\"You are a historian specializing in the Peshwa era. Below is a document in Marathi. \n",
    "Your task is to:\n",
    "1. Memorize all the dates and their historical significance.\n",
    "2. Understand the cultural and linguistic intricacies of Marathi history.\n",
    "3. Analyze and explain the events and stories surrounding the Peshwa era.\n",
    "\n",
    "### Marathi:\n",
    "{}\n",
    "\n",
    "### Historical Analysis:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # End-of-sequence token\n",
    "\n",
    "# Function to format the prompts\n",
    "def formatting_prompts_func(examples):\n",
    "    marathi_texts = examples[\"Peshwaai\"]\n",
    "    outputs = [\"\"] * len(marathi_texts)  # Placeholder for historical analysis\n",
    "\n",
    "    formatted_texts = []\n",
    "    for marathi_text, output in zip(marathi_texts, outputs):\n",
    "        formatted_text = custom_prompt.format(marathi_text, output) + EOS_TOKEN\n",
    "        formatted_texts.append(formatted_text)\n",
    "\n",
    "    return {\"texts\": formatted_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f931c10dd648198b8fbae6235a4340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7446 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Apply the `map` function to the subset\n",
    "dataset_peshwe = dataset_peshwe.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3918bce6aeb3494791f7ea8cd3040509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"texts\"]:   0%|          | 0/7446 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "from unsloth import is_bfloat16_supported\n",
    "# import lightning.pytorch as pl\n",
    "# from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move to GPU or CPU as required\n",
    "\n",
    "#Checkpointing logic here.\n",
    "# check_callback = ModelCheckpoint(\n",
    "#     dirpath = \"model_checkpoint\",\n",
    "#     filename=\"model-{epoch}-{step}-{global_step}\",\n",
    "#     save_top_k = -1,\n",
    "#     every_n_train_steps = 50,\n",
    "# )\n",
    "\n",
    "# Define trainer with consistent precision and multi-GPU support\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  \n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset_peshwe,  # Processed dataset\n",
    "    dataset_text_field=\"texts\",  # Combined 'text' column\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=1,\n",
    "    packing=False,  # Can make training faster for short sequences\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=8,\n",
    "        num_train_epochs=3,\n",
    "        warmup_steps=10,\n",
    "        max_steps=800,\n",
    "        learning_rate=5e-6,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        save_steps=50,  # <-- Save every 50 steps\n",
    "        save_total_limit=5,  # Keep last 5 checkpoints only\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.05,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=2802,\n",
    "        output_dir=\"outputs_peshwe_cpt\",\n",
    "        save_strategy=\"steps\",  # Important!\n",
    "        logging_dir=\"logs_peshwe_ckpt\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 7,446 | Num Epochs = 4 | Total steps = 800\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 8 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 273,825,792/12,000,000,000 (2.28% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [800/800 2:34:40, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.491800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.143900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.782100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.293200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.038800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.798000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.506900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.298500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.189900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.097000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.045200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.040700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.998800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.016400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.993800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.986100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.984300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.953900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.962900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.977800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.961300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.967100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.959700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.987200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.951600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.959900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.949200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.958400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.944600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.936900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.950100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.927900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.925400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.930700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.949100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.920900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.916300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.933500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.919400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.944200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.924800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.906300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.904500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.911500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.926100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.918300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.930800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.970200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.917600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.945200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.927100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.915100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.895100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.910200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.898900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.907900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.921400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.905400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.930500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.923100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.907200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.913800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.917500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.913500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.927900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=800, training_loss=1.1134182286262513, metrics={'train_runtime': 9292.1517, 'train_samples_per_second': 2.755, 'train_steps_per_second': 0.086, 'total_flos': 1.9266685807744512e+17, 'train_loss': 1.1134182286262513})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training after the interruption occurred due to restarting of the studio.\n",
    "trainer.train(resume_from_checkpoint='provide/path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text (Marathi):\n",
      "‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§è‡§ï ‡§á‡§§‡§ø‡§π‡§æ‡§∏‡§ï‡§æ‡§∞ ‡§Ü‡§π‡§æ‡§§ ‡§Ü‡§£‡§ø ‡§§‡•Å‡§Æ‡§ö‡•á ‡§∏‡§Ç‡§∂‡•ã‡§ß‡§® ‡§™‡•á‡§∂‡§µ‡§æ‡§à ‡§ï‡§æ‡§≤‡§ñ‡§Ç‡§°‡§æ‡§µ‡§∞ ‡§Ü‡§π‡•á. \n",
      "‡§ñ‡§æ‡§≤‡•Ä ‡§¶‡§ø‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§â‡§¶‡§æ‡§π‡§∞‡§£‡§æ‡§Ç‡§™‡•ç‡§∞‡§Æ‡§æ‡§£‡•á ‡§â‡§§‡•ç‡§§‡§∞ ‡§∏‡§µ‡§ø‡§∏‡•ç‡§§‡§∞ ‡§Ü‡§£‡§ø ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä‡§™‡•Ç‡§∞‡•ç‡§£ ‡§¶‡•ç‡§Ø‡§æ:\n",
      "\n",
      "‡§â‡§¶‡§æ‡§π‡§∞‡§£ ‡•ß:\n",
      "‡§µ‡§ø‡§∑‡§Ø: ‡§®‡§æ‡§®‡§æ ‡§´‡§°‡§£‡§µ‡•Ä‡§∏‡§æ‡§Ç‡§ö‡•á ‡§ó‡•Å‡§™‡•ç‡§§ ‡§∞‡§æ‡§ú‡§ï‡§æ‡§∞‡§£\n",
      "‡§∏‡§µ‡§ø‡§∏‡•ç‡§§‡§∞ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä: ‡§®‡§æ‡§®‡§æ ‡§´‡§°‡§£‡§µ‡•Ä‡§∏ ‡§π‡•á ‡§ï‡•á‡§µ‡§≥ ‡§™‡•á‡§∂‡§µ‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•á ‡§µ‡§ø‡§∂‡•ç‡§µ‡§æ‡§∏‡•Ç ‡§®‡§∏‡•Ç‡§® ‡§§‡•ç‡§Ø‡§æ‡§Ç‡§®‡•Ä '‡§¨‡§æ‡§∞‡§≠‡§æ‡§à ‡§Æ‡§Ç‡§°‡§≥‡§æ'‡§ö‡•ç‡§Ø‡§æ ‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ‡§æ‡§§‡•Ç‡§® ‡§™‡•á‡§∂‡§µ‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•Ä ‡§∏‡§§‡•ç‡§§‡§æ ‡§Ö‡§¨‡§æ‡§ß‡§ø‡§§ ‡§†‡•á‡§µ‡§£‡•ç‡§Ø‡§æ‡§ö‡§æ ‡§™‡•ç‡§∞‡§Ø‡§§‡•ç‡§® ‡§ï‡•á‡§≤‡§æ ‡§π‡•ã‡§§‡§æ. ...\n",
      "\n",
      "‡§â‡§¶‡§æ‡§π‡§∞‡§£ ‡•®:\n",
      "‡§µ‡§ø‡§∑‡§Ø: ‡§Æ‡§æ‡§ß‡§µ‡§∞‡§æ‡§µ ‡§™‡•á‡§∂‡§µ‡•ç‡§Ø‡§æ‡§Ç‡§ö‡§æ ‡§Ü‡§∞‡•ã‡§ó‡•ç‡§Ø‡§æ‡§µ‡§∞ ‡§ù‡§æ‡§≤‡•á‡§≤‡§æ ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ\n",
      "‡§∏‡§µ‡§ø‡§∏‡•ç‡§§‡§∞ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä: ‡§Æ‡§æ‡§ß‡§µ‡§∞‡§æ‡§µ ‡§™‡•á‡§∂‡§µ‡•á ‡§π‡•á ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§æ‡§® ‡§π‡•ã‡§§‡•á. ‡§Æ‡§æ‡§§‡•ç‡§∞ ‡§§‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ ‡§Ö‡§≤‡•ç‡§™ ‡§µ‡§Ø‡§æ‡§§ ‡§Æ‡•É‡§§‡•ç‡§Ø‡•Ç‡§ö‡•á ‡§ï‡§æ‡§∞‡§£ ‡§∞‡§æ‡§ú‡§ï‡•Ä‡§Ø ‡§§‡§£‡§æ‡§µ, ‡§ò‡§∞‡§ó‡•Å‡§§‡•Ä ‡§∏‡§Ç‡§ò‡§∞‡•ç‡§∑ ‡§Ü‡§£‡§ø ‡§∏‡§æ‡§§‡§§‡•ç‡§Ø‡§æ‡§®‡•á ‡§ù‡§æ‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§≤‡§¢‡§æ‡§Ø‡§æ‡§Ç‡§Æ‡•Å‡§≥‡•á ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£ ‡§ù‡§æ‡§≤‡•á‡§≤‡•á ‡§Ü‡§∞‡•ã‡§ó‡•ç‡§Ø‡§æ‡§ö‡•á ‡§¨‡§ø‡§ò‡§æ‡§° ‡§π‡•á ‡§π‡•ã‡§§‡•á...\n",
      "\n",
      "---\n",
      "\n",
      "‡§Ü‡§§‡§æ ‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§µ‡§ø‡§∑‡§Ø‡§æ‡§µ‡§∞ ‡§â‡§§‡•ç‡§§‡§∞ ‡§≤‡§ø‡§π‡§æ:\n",
      "\n",
      "‡§µ‡§ø‡§∑‡§Ø: ‡§™‡•Å‡§£‡•ç‡§Ø‡§æ‡§§‡•Ä‡§≤ ‡§∂‡§®‡§ø‡§µ‡§æ‡§∞ ‡§µ‡§æ‡§°‡•ç‡§Ø‡§æ‡§ö‡•á ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ ‡§ï‡§æ‡§Ø ‡§Ü‡§π‡•á?\n",
      "‡§∏‡§µ‡§ø‡§∏‡•ç‡§§‡§∞ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä: ‡§™‡•Å‡§£‡•á ‡§∂‡§π‡§∞‡§æ‡§ö‡•Ä ‡§∂‡§æ‡§® ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§∂‡§®‡§ø‡§µ‡§æ‡§∞‡§µ‡§æ‡§°‡§æ! ‡§Æ‡§∞‡§æ‡§†‡§æ ‡§∏‡§æ‡§Æ‡•ç‡§∞‡§æ‡§ú‡•ç‡§Ø‡§æ‡§ö‡•Ä ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§Æ‡•ç‡§π‡§£‡•Ç‡§® ‡§¶‡§ø‡§≤‡•á‡§≤‡•á ‡§Ø‡§æ ‡§µ‡§æ‡§°‡•ç‡§Ø‡§æ‡§ö‡§Ç ‡§Æ‡§π‡§§‡•ç‡§µ ‡§ñ‡•Ç‡§™ ‡§Æ‡•ã‡§†‡§Ç ‡§π‡•ã‡§§‡§Ç. ‡•ß‡•≠‡•©‡•® ‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§¨‡§æ‡§≥‡§æ‡§ú‡•Ä ‡§µ‡§ø‡§∂‡•ç‡§µ‡§®‡§æ‡§• ‡§≠‡§ü‡•ç‡§ü ‡§Ø‡§æ‡§Ç‡§®‡•Ä ‡§∞‡§ò‡•Å‡§®‡§æ‡§•‡§∞‡§æ‡§Ø‡§æ‡§Ç‡§∏‡§æ‡§†‡•Ä ‡§π‡§æ ‡§∏‡•Å‡§Ç‡§¶‡§∞ ‡§™‡§∞‡§ø‡§∏‡§∞ ‡§®‡§ø‡§µ‡§°‡§≤‡§æ ‡§§‡•á‡§µ‡•ç‡§π‡§æ‡§™‡§æ‡§∏‡•Ç‡§®‡§ö ‡§Ø‡§æ‡§ö‡•Ä ‡§∏‡•Å‡§∞‡•Å‡§µ‡§æ‡§§ ‡§ù‡§æ‡§≤‡•Ä. ‡§∏‡•Å‡§∞‡•Å‡§µ‡§æ‡§§‡•Ä‡§≤‡§æ ‡§á‡§•‡•á ‡§´‡§ï‡•ç‡§§ ‡§µ‡§ø‡§∂‡•ç‡§∞‡§æ‡§Æ‡§ó‡•É‡§π ‡§®‡§µ‡•ç‡§π‡§§‡§Ç ‡§§‡§∞ ‡§∂‡•ç‡§∞‡•Ä‡§Æ‡§Ç‡§§ ‡§õ‡§§‡•ç‡§∞‡§™‡§§‡•Ä ‡§∂‡§æ‡§π‡•Ç‡§Ç‡§®‡•Ä ‡§á‡§•‡§≤‡•Ä ‡§ú‡§π‡§æ‡§ó‡•Ä‡§∞ ‡§Ü‡§™‡§≤‡•ç‡§Ø‡§æ ‡§§‡§æ‡§¨‡•ç‡§Ø‡§æ‡§§ ‡§ò‡•á‡§ä‡§® ‡§Ø‡§æ ‡§ú‡§æ‡§ó‡•á‡§ö‡§æ ‡§µ‡§ø‡§ï‡§æ‡§∏ ‡§ï‡§∞‡§£‡•ç‡§Ø‡§æ‡§§ ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§æ‡§≠‡§∞‡§æ‡§ä ‡§Ø‡•ã‡§ó‡§¶‡§æ‡§® ‡§¶‡§ø‡§≤‡§Ç. ‡•ß‡•Æ ‡§µ‡•ç‡§Ø‡§æ ‡§∂‡§§‡§ï‡§æ‡§§ ‡§π‡•Ä ‡§ú‡§æ‡§ó‡§æ ‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡•Ä, ‡§ï‡§≤‡§æ ‡§Ü‡§£‡§ø ‡§∏‡§æ‡§π‡§ø‡§§‡•ç‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä ‡§ï‡•á‡§Ç‡§¶‡•ç‡§∞ ‡§¨‡§®‡§≤‡•Ä ‡§π‡•ã‡§§‡•Ä. ‡§Ö‡§®‡•á‡§ï ‡§∏‡§æ‡§π‡§ø‡§§‡•ç‡§Ø‡§ø‡§ï, ‡§ï‡§µ‡•Ä, ‡§ï‡§≤‡§æ‡§ï‡§æ‡§∞ ‡§Ø‡§æ‡§ö ‡§µ‡§æ‡§°‡•ç‡§Ø‡§æ‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§è‡§ï‡§§‡•ç‡§∞ ‡§Ø‡•á‡§§ ‡§Ö‡§∏‡§§ ‡§§‡•ç‡§Ø‡§æ‡§Æ‡•Å‡§≥‡•á ‡§Ø‡§æ ‡§†‡§ø‡§ï‡§æ‡§£‡§æ‡§≤‡§æ ‡§∏‡§æ‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø‡§ï ‡§Ü‡§∂‡•ç‡§∞‡§Ø‡§∏‡•ç‡§•‡§æ‡§® ‡§Æ‡§æ‡§®‡§≤‡•á ‡§ú‡§æ‡§§‡•á. ‡§™‡•á‡§∂‡§µ‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ ‡§ï‡§æ‡§≥‡§æ‡§§ ‡§∂‡§®‡§ø‡§µ‡§æ‡§∞‡§µ‡§æ‡§°‡•ç‡§Ø‡§æ‡§§ ‡§∂‡§æ‡§π‡•Ä ‡§¶‡§∞‡§¨‡§æ‡§∞‡§æ‡§∏‡§æ‡§∞‡§ñ‡§æ ‡§•‡§æ‡§ü‡§Æ‡§æ‡§ü‡§æ ‡§Ö‡§∏‡•á. ‡§µ‡§ø‡§µ‡§ø‡§ß ‡§™‡•ç‡§∞‡§æ‡§Ç‡§§‡§∏‡•ç‡§•‡•Ä‡§π‡•Ç‡§® ‡§Ü‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§∏‡§∞‡§¶‡§æ‡§∞ ‡§≤‡•ã‡§ï‡§æ‡§Ç‡§ö‡§Ç ‡§®‡§ø‡§µ‡§æ‡§∏‡§∏‡•ç‡§•‡§æ‡§® ‡§Æ‡•ç‡§π‡§£‡§ú‡•á‡§ö ‡§∂‡§®‡§ø‡§µ‡§æ‡§∞‡§µ‡§æ‡§°‡•ç‡§Ø‡§æ‡§ï‡§°‡•á ‡§π‡•ã‡§§. ‡§Æ‡•ç‡§π‡§£‡•Ç‡§®‡§ö ‡§∂‡§®‡§ø‡§µ‡§æ‡§∞‡§µ‡§æ‡§°‡•á ‡§ï‡•ã‡§£‡§§‡•ç‡§Ø‡§æ‡§π‡•Ä ‡§ê‡§§‡§ø‡§π‡§æ‡§∏‡§ø‡§ï ‡§ò‡§ü‡§®‡•á‡§™‡•á‡§ï‡•ç‡§∑‡§æ ‡§ï‡§Æ‡•Ä ‡§®‡§æ‡§π‡•Ä.\n",
      "\n",
      "Inference Time: 31.2549 seconds\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import time\n",
    "\n",
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Marathi prompt for generation\n",
    "# Marathi prompt updated\n",
    "custom_prompt = \"\"\"‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§è‡§ï ‡§á‡§§‡§ø‡§π‡§æ‡§∏‡§ï‡§æ‡§∞ ‡§Ü‡§π‡§æ‡§§ ‡§Ü‡§£‡§ø ‡§§‡•Å‡§Æ‡§ö‡•á ‡§∏‡§Ç‡§∂‡•ã‡§ß‡§® ‡§™‡•á‡§∂‡§µ‡§æ‡§à ‡§ï‡§æ‡§≤‡§ñ‡§Ç‡§°‡§æ‡§µ‡§∞ ‡§Ü‡§π‡•á. \n",
    "‡§ñ‡§æ‡§≤‡•Ä ‡§¶‡§ø‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§â‡§¶‡§æ‡§π‡§∞‡§£‡§æ‡§Ç‡§™‡•ç‡§∞‡§Æ‡§æ‡§£‡•á ‡§â‡§§‡•ç‡§§‡§∞ ‡§∏‡§µ‡§ø‡§∏‡•ç‡§§‡§∞ ‡§Ü‡§£‡§ø ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä‡§™‡•Ç‡§∞‡•ç‡§£ ‡§¶‡•ç‡§Ø‡§æ:\n",
    "\n",
    "‡§â‡§¶‡§æ‡§π‡§∞‡§£ ‡•ß:\n",
    "‡§µ‡§ø‡§∑‡§Ø: ‡§®‡§æ‡§®‡§æ ‡§´‡§°‡§£‡§µ‡•Ä‡§∏‡§æ‡§Ç‡§ö‡•á ‡§ó‡•Å‡§™‡•ç‡§§ ‡§∞‡§æ‡§ú‡§ï‡§æ‡§∞‡§£\n",
    "‡§∏‡§µ‡§ø‡§∏‡•ç‡§§‡§∞ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä: ‡§®‡§æ‡§®‡§æ ‡§´‡§°‡§£‡§µ‡•Ä‡§∏ ‡§π‡•á ‡§ï‡•á‡§µ‡§≥ ‡§™‡•á‡§∂‡§µ‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•á ‡§µ‡§ø‡§∂‡•ç‡§µ‡§æ‡§∏‡•Ç ‡§®‡§∏‡•Ç‡§® ‡§§‡•ç‡§Ø‡§æ‡§Ç‡§®‡•Ä '‡§¨‡§æ‡§∞‡§≠‡§æ‡§à ‡§Æ‡§Ç‡§°‡§≥‡§æ'‡§ö‡•ç‡§Ø‡§æ ‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ‡§æ‡§§‡•Ç‡§® ‡§™‡•á‡§∂‡§µ‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•Ä ‡§∏‡§§‡•ç‡§§‡§æ ‡§Ö‡§¨‡§æ‡§ß‡§ø‡§§ ‡§†‡•á‡§µ‡§£‡•ç‡§Ø‡§æ‡§ö‡§æ ‡§™‡•ç‡§∞‡§Ø‡§§‡•ç‡§® ‡§ï‡•á‡§≤‡§æ ‡§π‡•ã‡§§‡§æ. ...\n",
    "\n",
    "‡§â‡§¶‡§æ‡§π‡§∞‡§£ ‡•®:\n",
    "‡§µ‡§ø‡§∑‡§Ø: ‡§Æ‡§æ‡§ß‡§µ‡§∞‡§æ‡§µ ‡§™‡•á‡§∂‡§µ‡•ç‡§Ø‡§æ‡§Ç‡§ö‡§æ ‡§Ü‡§∞‡•ã‡§ó‡•ç‡§Ø‡§æ‡§µ‡§∞ ‡§ù‡§æ‡§≤‡•á‡§≤‡§æ ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ\n",
    "‡§∏‡§µ‡§ø‡§∏‡•ç‡§§‡§∞ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä: ‡§Æ‡§æ‡§ß‡§µ‡§∞‡§æ‡§µ ‡§™‡•á‡§∂‡§µ‡•á ‡§π‡•á ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§æ‡§® ‡§π‡•ã‡§§‡•á. ‡§Æ‡§æ‡§§‡•ç‡§∞ ‡§§‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ ‡§Ö‡§≤‡•ç‡§™ ‡§µ‡§Ø‡§æ‡§§ ‡§Æ‡•É‡§§‡•ç‡§Ø‡•Ç‡§ö‡•á ‡§ï‡§æ‡§∞‡§£ ‡§∞‡§æ‡§ú‡§ï‡•Ä‡§Ø ‡§§‡§£‡§æ‡§µ, ‡§ò‡§∞‡§ó‡•Å‡§§‡•Ä ‡§∏‡§Ç‡§ò‡§∞‡•ç‡§∑ ‡§Ü‡§£‡§ø ‡§∏‡§æ‡§§‡§§‡•ç‡§Ø‡§æ‡§®‡•á ‡§ù‡§æ‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§≤‡§¢‡§æ‡§Ø‡§æ‡§Ç‡§Æ‡•Å‡§≥‡•á ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£ ‡§ù‡§æ‡§≤‡•á‡§≤‡•á ‡§Ü‡§∞‡•ã‡§ó‡•ç‡§Ø‡§æ‡§ö‡•á ‡§¨‡§ø‡§ò‡§æ‡§° ‡§π‡•á ‡§π‡•ã‡§§‡•á...\n",
    "\n",
    "---\n",
    "\n",
    "‡§Ü‡§§‡§æ ‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§µ‡§ø‡§∑‡§Ø‡§æ‡§µ‡§∞ ‡§â‡§§‡•ç‡§§‡§∞ ‡§≤‡§ø‡§π‡§æ:\n",
    "\n",
    "‡§µ‡§ø‡§∑‡§Ø: {}\n",
    "‡§∏‡§µ‡§ø‡§∏‡•ç‡§§‡§∞ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä:\"\"\"\n",
    "\n",
    "\n",
    "# Input topic (modify as needed)\n",
    "input_text = \"‡§™‡•Å‡§£‡•ç‡§Ø‡§æ‡§§‡•Ä‡§≤ ‡§∂‡§®‡§ø‡§µ‡§æ‡§∞ ‡§µ‡§æ‡§°‡•ç‡§Ø‡§æ‡§ö‡•á ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ ‡§ï‡§æ‡§Ø ‡§Ü‡§π‡•á?\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer(\n",
    "    [custom_prompt.format(input_text)],  \n",
    "    padding=True, truncation=True, return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate output\n",
    "start_time = time.time()\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=1024,  \n",
    "    do_sample=True,  \n",
    "    temperature=0.7,  \n",
    "    top_p=0.9,  \n",
    "    repetition_penalty=1.3,  \n",
    "    no_repeat_ngram_size=3  \n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the output\n",
    "print(\"\\nGenerated Text (Marathi):\")\n",
    "print(generated_text)\n",
    "print(f\"\\nInference Time: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
