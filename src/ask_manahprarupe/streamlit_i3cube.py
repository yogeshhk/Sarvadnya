# import os
# from dotenv import load_dotenv
# import streamlit as st
# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings
# from llama_index.embeddings.huggingface import HuggingFaceEmbedding
# from llama_index.core.node_parser import SimpleNodeParser

# # ‚úÖ Load environment variables
# load_dotenv()
# st.set_page_config(page_title="RAG Marathi Chatbot", layout="wide")
# st.title("ü§ñ Marathi RAG Chatbot (I3Cube)")

# st.markdown("Ask questions in **Marathi**, and the bot will retrieve answers from your documents.")

# # ‚úÖ Disable LLM for retrieval-only mode
# Settings.llm = None
# Settings.embed_model = HuggingFaceEmbedding(
#     model_name="l3cube-pune/marathi-sentence-bert-nli"
# )

# # ‚úÖ Cache the RAG system for faster reload
# @st.cache_resource(show_spinner=True)
# def load_rag(data_directory="./data"):
#     if not os.path.exists(data_directory) or len(os.listdir(data_directory)) == 0:
#         st.error(f"No files found in `{data_directory}`. Please add `.txt` or `.tex` files.")
#         return None

#     st.info(f"üìÇ Loading documents from `{data_directory}` ...")
#     documents = SimpleDirectoryReader(
#         data_directory,
#         recursive=True,
#         filename_as_id=True,
#         required_exts=[".txt", ".tex"]
#     ).load_data()
#     st.success(f"‚úÖ Loaded {len(documents)} documents!")

#     # Parse documents into chunks
#     parser = SimpleNodeParser.from_defaults(chunk_size=300, chunk_overlap=50)
#     nodes = parser.get_nodes_from_documents(documents)

#     # Create vector index
#     index = VectorStoreIndex(nodes)

#     # Return query engine with top 3 retrievals
#     return index.as_query_engine(
#         similarity_top_k=3,
#         llm=None,
#         response_mode="no_text"
#     )

# # ‚úÖ Initialize RAG
# query_engine = load_rag("./data")

# if query_engine:
#     # ‚úÖ Input Box
#     question = st.text_input("‚ùì ‡§§‡•Å‡§Æ‡§ö‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§µ‡§ø‡§ö‡§æ‡§∞‡§æ‡§É", placeholder="‡§â‡§¶‡§æ: ‡§∏‡•ç‡§µ‡•â‡§ü ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§ï‡§æ‡§Ø?")
    
#     if question:
#         with st.spinner("üîç ‡§∂‡•ã‡§ß‡§§ ‡§Ü‡§π‡•á..."):
#             response = query_engine.query(question)

#             if not response.source_nodes:
#                 st.warning("‡§Æ‡§æ‡§´ ‡§ï‡§∞‡§æ, ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§∏‡§æ‡§™‡§°‡§≤‡•Ä ‡§®‡§æ‡§π‡•Ä.")
#             else:
#                 st.subheader("‚úÖ ‡§â‡§§‡•ç‡§§‡§∞:")
#                 for i, node in enumerate(response.source_nodes, start=1):
#                     score = round(node.score or 0, 3)
#                     text = node.node.text.strip().replace("\n", " ")
#                     short_text = text[:300] + "..." if len(text) > 300 else text
#                     st.markdown(f"**üîπ [Score: {score}]** {short_text}")
import os
from dotenv import load_dotenv
import streamlit as st
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.node_parser import SimpleNodeParser

# ‚úÖ Load environment variables
load_dotenv()
st.set_page_config(page_title="RAG Marathi Chatbot", layout="wide")

# ‚úÖ Inject Custom CSS
with open("static/style.css") as css:
    st.markdown(f"<style>{css.read()}</style>", unsafe_allow_html=True)

# ‚úÖ Disable LLM
Settings.llm = None
Settings.embed_model = HuggingFaceEmbedding(
    model_name="l3cube-pune/marathi-sentence-bert-nli"
)

# ‚úÖ Cache RAG
@st.cache_resource(show_spinner=True)
def load_rag(data_directory="./data"):
    if not os.path.exists(data_directory) or len(os.listdir(data_directory)) == 0:
        st.error(f"No files found in `{data_directory}`.")
        return None

    documents = SimpleDirectoryReader(
        data_directory,
        recursive=True,
        filename_as_id=True,
        required_exts=[".txt", ".tex"]
    ).load_data()

    parser = SimpleNodeParser.from_defaults(chunk_size=300, chunk_overlap=50)
    nodes = parser.get_nodes_from_documents(documents)
    index = VectorStoreIndex(nodes)

    return index.as_query_engine(similarity_top_k=3, llm=None, response_mode="no_text")

query_engine = load_rag("./data")

# ‚úÖ Custom Chat UI
st.markdown("<h1>ü§ñ Marathi RAG Chatbot (I3Cube)</h1>", unsafe_allow_html=True)
st.markdown('<div class="chat-container">', unsafe_allow_html=True)

question = st.text_input("‡§§‡•Å‡§Æ‡§ö‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§®:", placeholder="‡§â‡§¶‡§æ: ‡§∏‡•ç‡§µ‡•â‡§ü ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§ï‡§æ‡§Ø?")

submit = st.button("üîç ‡§∂‡•ã‡§ß‡§æ")

if submit and query_engine:
    response = query_engine.query(question)

    if not response.source_nodes:
        st.markdown('<div class="answer-box">‚ùå ‡§Æ‡§æ‡§´ ‡§ï‡§∞‡§æ, ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§∏‡§æ‡§™‡§°‡§≤‡•Ä ‡§®‡§æ‡§π‡•Ä.</div>', unsafe_allow_html=True)
    else:
        st.markdown('<div class="answer-box">', unsafe_allow_html=True)
        for i, node in enumerate(response.source_nodes, start=1):
            score = round(node.score or 0, 3)
            text = node.node.text.strip().replace("\n", " ")
            short_text = text[:300] + "..." if len(text) > 300 else text
            st.markdown(f'<div class="answer-item"><b>Score: {score}</b> ‚Äî {short_text}</div>', unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)

st.markdown('</div>', unsafe_allow_html=True)
