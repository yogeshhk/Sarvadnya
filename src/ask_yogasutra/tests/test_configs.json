{
  "configurations": {
    "baseline_fast": {
      "name": "baseline_fast",
      "description": "Fast model with default settings for quick testing",
      "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
      "groq_model": "llama-3.1-8b-instant",
      "temperature": 0.1,
      "max_tokens": 512,
      "chunk_size": 512,
      "chunk_overlap": 20,
      "max_triplets_per_chunk": 5,
      "conversation_mode": true,
      "chat_mode": "condense_plus_context",
      "response_mode": "compact",
      "graph_file": "data/graph_small.json",
      "use_cached_index": true
    },
    "baseline_quality": {
      "name": "baseline_quality",
      "description": "Higher quality model with more tokens for better responses",
      "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
      "groq_model": "llama-3.3-70b-versatile",
      "temperature": 0.1,
      "max_tokens": 1024,
      "chunk_size": 512,
      "chunk_overlap": 20,
      "max_triplets_per_chunk": 5,
      "conversation_mode": true,
      "chat_mode": "condense_plus_context",
      "response_mode": "compact",
      "graph_file": "data/graph_small.json",
      "use_cached_index": true
    },
    "query_mode": {
      "name": "query_mode",
      "description": "Non-conversational mode with no context memory",
      "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
      "groq_model": "llama-3.1-8b-instant",
      "temperature": 0.1,
      "max_tokens": 512,
      "chunk_size": 512,
      "chunk_overlap": 20,
      "max_triplets_per_chunk": 5,
      "conversation_mode": false,
      "chat_mode": null,
      "response_mode": "compact",
      "graph_file": "data/graph_small.json",
      "use_cached_index": true
    },
    "larger_chunks": {
      "name": "larger_chunks",
      "description": "Larger chunk size for better context retention",
      "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
      "groq_model": "llama-3.1-8b-instant",
      "temperature": 0.1,
      "max_tokens": 512,
      "chunk_size": 1024,
      "chunk_overlap": 50,
      "max_triplets_per_chunk": 5,
      "conversation_mode": true,
      "chat_mode": "condense_plus_context",
      "response_mode": "compact",
      "graph_file": "data/graph_small.json",
      "use_cached_index": true
    },
    "higher_temperature": {
      "name": "higher_temperature",
      "description": "Higher temperature for more creative responses",
      "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
      "groq_model": "llama-3.1-8b-instant",
      "temperature": 0.3,
      "max_tokens": 512,
      "chunk_size": 512,
      "chunk_overlap": 20,
      "max_triplets_per_chunk": 5,
      "conversation_mode": true,
      "chat_mode": "condense_plus_context",
      "response_mode": "compact",
      "graph_file": "data/graph_small.json",
      "use_cached_index": true
    },
    "better_embeddings": {
      "name": "better_embeddings",
      "description": "Better embedding model for improved semantic understanding",
      "embedding_model": "sentence-transformers/all-mpnet-base-v2",
      "groq_model": "llama-3.1-8b-instant",
      "temperature": 0.1,
      "max_tokens": 512,
      "chunk_size": 512,
      "chunk_overlap": 20,
      "max_triplets_per_chunk": 5,
      "conversation_mode": true,
      "chat_mode": "condense_plus_context",
      "response_mode": "compact",
      "graph_file": "data/graph_small.json",
      "use_cached_index": true
    },
    "full_graph": {
      "name": "full_graph",
      "description": "Using full graph instead of small sample for comprehensive testing",
      "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
      "groq_model": "llama-3.1-8b-instant",
      "temperature": 0.1,
      "max_tokens": 512,
      "chunk_size": 512,
      "chunk_overlap": 20,
      "max_triplets_per_chunk": 5,
      "conversation_mode": true,
      "chat_mode": "condense_plus_context",
      "response_mode": "compact",
      "graph_file": "data/graph.json",
      "use_cached_index": true
    },
    "tree_summarize": {
      "name": "tree_summarize",
      "description": "Using tree_summarize response mode for detailed answers",
      "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
      "groq_model": "llama-3.1-8b-instant",
      "temperature": 0.1,
      "max_tokens": 1024,
      "chunk_size": 512,
      "chunk_overlap": 20,
      "max_triplets_per_chunk": 5,
      "conversation_mode": true,
      "chat_mode": "condense_plus_context",
      "response_mode": "tree_summarize",
      "graph_file": "data/graph_small.json",
      "use_cached_index": true
    },
    "refine_mode": {
      "name": "refine_mode",
      "description": "Using refine response mode for iterative improvement",
      "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
      "groq_model": "llama-3.1-8b-instant",
      "temperature": 0.1,
      "max_tokens": 1024,
      "chunk_size": 512,
      "chunk_overlap": 20,
      "max_triplets_per_chunk": 5,
      "conversation_mode": true,
      "chat_mode": "condense_plus_context",
      "response_mode": "refine",
      "graph_file": "data/graph_small.json",
      "use_cached_index": true
    },
    "linearrag_baseline": {
      "name": "linearrag_baseline",
      "description": "Linear RAG baseline configuration for comparison",
      "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
      "groq_model": "llama-3.1-8b-instant",
      "temperature": 0.1,
      "max_tokens": 512,
      "chunk_size": 512,
      "chunk_overlap": 20,
      "max_triplets_per_chunk": 5,
      "conversation_mode": false,
      "chat_mode": null,
      "response_mode": "compact",
      "graph_file": "data/graph_small.json",
      "backend_type": "linearrag",
      "use_cached_index": true
    },
    "high_precision": {
      "name": "high_precision",
      "description": "High precision configuration with lower temperature and larger chunks",
      "embedding_model": "sentence-transformers/all-mpnet-base-v2",
      "groq_model": "llama-3.3-70b-versatile",
      "temperature": 0.05,
      "max_tokens": 1024,
      "chunk_size": 1024,
      "chunk_overlap": 50,
      "max_triplets_per_chunk": 10,
      "conversation_mode": true,
      "chat_mode": "condense_plus_context",
      "response_mode": "tree_summarize",
      "graph_file": "data/graph_small.json",
      "use_cached_index": true
    },
    "context_chat_mode": {
      "name": "context_chat_mode",
      "description": "Using context chat mode for direct retrieval-based responses",
      "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
      "groq_model": "llama-3.1-8b-instant",
      "temperature": 0.1,
      "max_tokens": 512,
      "chunk_size": 512,
      "chunk_overlap": 20,
      "max_triplets_per_chunk": 5,
      "conversation_mode": true,
      "chat_mode": "context",
      "response_mode": "compact",
      "graph_file": "data/graph_small.json",
      "use_cached_index": true
    },
    "condense_question_mode": {
      "name": "condense_question_mode",
      "description": "Using condense_question chat mode for standalone question generation",
      "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
      "groq_model": "llama-3.1-8b-instant",
      "temperature": 0.1,
      "max_tokens": 512,
      "chunk_size": 512,
      "chunk_overlap": 20,
      "max_triplets_per_chunk": 5,
      "conversation_mode": true,
      "chat_mode": "condense_question",
      "response_mode": "compact",
      "graph_file": "data/graph_small.json",
      "use_cached_index": true
    },
    "no_cache_rebuild": {
      "name": "no_cache_rebuild",
      "description": "Force rebuild index instead of using cached version",
      "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
      "groq_model": "llama-3.1-8b-instant",
      "temperature": 0.1,
      "max_tokens": 512,
      "chunk_size": 512,
      "chunk_overlap": 20,
      "max_triplets_per_chunk": 5,
      "conversation_mode": true,
      "chat_mode": "condense_plus_context",
      "response_mode": "compact",
      "graph_file": "data/graph_small.json",
      "use_cached_index": false
    }
  }
}
